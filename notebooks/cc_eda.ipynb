{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn - Exploratory Data Analysis",
    "",
    "**Objective**: Understand what drives customer churn in a SaaS subscription business.",
    "",
    "This notebook performs a structured, business-focused EDA on 50,000 customer records, covering data quality, feature distributions, churn drivers, and a clean handoff to the modeling pipeline.",
    "",
    "**Workflow**: Data Loading -> Quality Checks -> Target Analysis -> Feature Analysis -> Outlier/Skew -> Modeling Handoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Data Loading",
    "",
    "Load the dataset using a robust path resolver that tries multiple locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "plt.rcParams[\"axes.titleweight\"] = \"bold\"\n",
    "plt.rcParams[\"font.size\"] = 11\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "print(\"Imports loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust data loader - tries multiple paths\n",
    "data_paths = [\n",
    "    Path(\"../data/customers.csv\"),\n",
    "    Path(\"./customers.csv\"),\n",
    "    Path(\"data/customers.csv\"),\n",
    "]\n",
    "\n",
    "df = None\n",
    "for p in data_paths:\n",
    "    if p.exists():\n",
    "        df = pd.read_csv(p)\n",
    "        print(f\"Loaded data from: {p.resolve()}\")\n",
    "        break\n",
    "\n",
    "if df is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find customers.csv. \"\n",
    "        \"Tried: \" + \", \".join(str(p) for p in data_paths)\n",
    "    )\n",
    "\n",
    "print(f\"Dataset shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading complete.** The dataset contains 50,000 rows and 19 columns. Each row represents a single customer with demographic, behavioral, billing, and churn status information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Checks",
    "",
    "Before any analysis, we audit the dataset for missing values, duplicates, data types, and target balance. These checks prevent silent errors downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a. Data types overview\n",
    "print(\"=\" * 50)\n",
    "print(\"DATA TYPES\")\n",
    "print(\"=\" * 50)\n",
    "print(df.dtypes.value_counts())\n",
    "print()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data types summary:** The dataset has a mix of integers, floats, and object (string) columns. Date columns (`signup_date`, `last_active`) are currently stored as strings and will need parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b. Missing value summary (sorted descending)\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_summary = pd.DataFrame({\n",
    "    \"Missing Count\": missing,\n",
    "    \"Missing %\": missing_pct\n",
    "}).sort_values(\"Missing Count\", ascending=False)\n",
    "missing_summary = missing_summary[missing_summary[\"Missing Count\"] > 0]\n",
    "\n",
    "if len(missing_summary) == 0:\n",
    "    print(\"No missing values found in the dataset.\")\n",
    "else:\n",
    "    print(\"Missing Value Summary:\")\n",
    "    display(missing_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Missing values:** The dataset is fully populated with no missing values across all 19 columns. No imputation strategy is needed for EDA, though the modeling pipeline should still include imputation as a safety net for production data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2c. Duplicate row check\n",
    "dup_count = df.duplicated().sum()\n",
    "print(f\"Duplicate rows: {dup_count:,} ({dup_count / len(df):.2%})\")\n",
    "if dup_count > 0:\n",
    "    print(\"WARNING: Consider removing duplicates before modeling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Duplicates:** Zero duplicate rows detected. The data integrity is solid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d. Target variable balance\n",
    "target_col = \"churned\"\n",
    "churn_counts = df[target_col].value_counts()\n",
    "churn_pct = df[target_col].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Target Distribution:\")\n",
    "print(f\"  Not Churned (0): {churn_counts[0]:,} ({churn_pct[0]:.1f}%)\")\n",
    "print(f\"  Churned     (1): {churn_counts[1]:,} ({churn_pct[1]:.1f}%)\")\n",
    "print(f\"  Imbalance Ratio: {churn_counts[0] / churn_counts[1]:.1f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Target balance:** The dataset has a 77/23 split (3.3:1 imbalance ratio). This is moderate imbalance - without correction, ML models may default to predicting the majority class (\"No Churn\") and achieve ~77% accuracy while catching zero actual churners. The modeling pipeline must use `class_weight='balanced'` or similar techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Target Variable Analysis",
    "",
    "Understanding the churn distribution is critical before modeling. A 77/23 split means the business loses roughly 1 in 4 customers - a significant revenue risk that justifies investing in prediction and intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# Count plot\n",
    "colors = [\"#2ecc71\", \"#e74c3c\"]\n",
    "churn_counts = df[target_col].value_counts()\n",
    "axes[0].bar([\"Retained\", \"Churned\"], churn_counts.values, color=colors, edgecolor=\"black\", linewidth=0.5)\n",
    "for i, v in enumerate(churn_counts.values):\n",
    "    axes[0].text(i, v + 300, f\"{v:,}\", ha=\"center\", fontweight=\"bold\")\n",
    "axes[0].set_title(\"Customer Churn Distribution\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Percentage pie\n",
    "axes[1].pie(churn_counts.values, labels=[\"Retained\", \"Churned\"], autopct=\"%1.1f%%\",\n",
    "            colors=colors, startangle=90, explode=(0, 0.05),\n",
    "            textprops={\"fontweight\": \"bold\"})\n",
    "axes[1].set_title(\"Churn Rate Breakdown\")\n",
    "\n",
    "plt.suptitle(\"Target Variable: Customer Churn\", fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** Out of 50,000 customers, approximately 11,700 (~23%) have churned. This is a meaningful churn rate - each percentage point of improved retention represents ~500 customers. Even a modest 5% lift in retention from a predictive model could save significant monthly recurring revenue (MRR). The imbalance is moderate enough that standard techniques (class weighting, threshold tuning) should handle it well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Numerical Feature Analysis",
    "",
    "We examine distributions, summary statistics, boxplots segmented by churn status, and correlations to identify features with predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dates for feature engineering\n",
    "for col in [\"signup_date\", \"last_active\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "# Derive tenure_days if both date columns exist\n",
    "if \"signup_date\" in df.columns and \"last_active\" in df.columns:\n",
    "    df[\"tenure_days\"] = (df[\"last_active\"] - df[\"signup_date\"]).dt.days\n",
    "\n",
    "# Identify numeric columns (exclude target)\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.drop(target_col).tolist()\n",
    "print(f\"Numerical features ({len(num_cols)}): {num_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4a. Summary statistics\n",
    "df[num_cols].describe().T.style.format(\"{:.2f}\").background_gradient(cmap=\"YlOrRd\", subset=[\"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary statistics:** The standard deviation column is highlighted to quickly spot high-variance features. Features like `tenure_days` and `monthly_fee` have wide ranges, while `payment_failures_180d` has a very low mean but nonzero max, indicating a heavily right-skewed distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4b. Distribution histograms colored by churn\n",
    "n_cols_plot = 3\n",
    "n_rows_plot = -(-len(num_cols) // n_cols_plot)\n",
    "fig, axes = plt.subplots(n_rows_plot, n_cols_plot, figsize=(6 * n_cols_plot, 4 * n_rows_plot))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    for label, color in [(0, \"#2ecc71\"), (1, \"#e74c3c\")]:\n",
    "        subset = df[df[target_col] == label][col].dropna()\n",
    "        axes[i].hist(subset, bins=30, alpha=0.6, color=color,\n",
    "                     label=\"Retained\" if label == 0 else \"Churned\", edgecolor=\"white\")\n",
    "    axes[i].set_title(col, fontweight=\"bold\")\n",
    "    axes[i].legend(fontsize=8)\n",
    "\n",
    "for j in range(len(num_cols), len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.suptitle(\"Numerical Feature Distributions by Churn Status\", fontsize=14, fontweight=\"bold\", y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution insights:** ",
    "- **Engagement features** (`sessions_per_week`, `avg_session_minutes`): Churned customers (red) tend to cluster at lower values, confirming that disengagement precedes churn.",
    "- **`days_since_last_login`**: Churned customers show a clear rightward shift - they haven't logged in recently. This is a strong signal but potentially **leaky** (a customer who churned naturally stops logging in).",
    "- **`payment_failures_180d`**: Most customers have zero failures, but the small group with failures churns at a higher rate.",
    "- **`age`** and **`monthly_fee`**: Distributions overlap significantly between churned/retained, suggesting weaker individual predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4c. Box plots by churn status\n",
    "fig, axes = plt.subplots(n_rows_plot, n_cols_plot, figsize=(6 * n_cols_plot, 4 * n_rows_plot))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    data_plot = df[[col, target_col]].dropna()\n",
    "    sns.boxplot(data=data_plot, x=target_col, y=col, ax=axes[i],\n",
    "                palette={0: \"#2ecc71\", 1: \"#e74c3c\"}, hue=target_col, legend=False)\n",
    "    axes[i].set_xticklabels([\"Retained\", \"Churned\"])\n",
    "    axes[i].set_title(col, fontweight=\"bold\")\n",
    "    axes[i].set_xlabel(\"\")\n",
    "\n",
    "for j in range(len(num_cols), len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.suptitle(\"Numerical Features: Box Plots by Churn Status\", fontsize=14, fontweight=\"bold\", y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Box plot insights:**",
    "- **Clear median separation** in `sessions_per_week`, `avg_session_minutes`, and `days_since_last_login` - these features have the strongest visual separation between churned and retained groups.",
    "- **Outliers** are visible in `payment_failures_180d` and `num_tickets_90d` - a few customers have extreme values. Tree-based models handle these naturally, but linear models may need capping or transformation.",
    "- **`discount_rate`** shows minimal difference between groups, suggesting it may be a weak predictor on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4d. Correlation heatmap (annotated)\n",
    "num_df = df[num_cols + [target_col]].select_dtypes(include=[np.number])\n",
    "corr = num_df.corr()\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap=\"RdBu_r\",\n",
    "            center=0, vmin=-1, vmax=1, linewidths=0.5,\n",
    "            cbar_kws={\"label\": \"Correlation Coefficient\"})\n",
    "plt.title(\"Feature Correlation Heatmap\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation heatmap insights:**",
    "- **Strongest positive correlation with churn**: `days_since_last_login` - customers who haven't logged in recently are more likely to churn (but this is a leaky feature).",
    "- **Strongest negative correlations with churn**: `sessions_per_week` and `avg_session_minutes` - higher engagement = lower churn.",
    "- **Inter-feature correlation**: `sessions_per_week` and `avg_session_minutes` are moderately correlated with each other (~0.3-0.5), which is expected - engaged users do both. This is not high enough to cause multicollinearity issues.",
    "- No pair exceeds |0.8|, so no features need to be dropped for redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4e. Correlation with churn (sorted bar chart)\n",
    "churn_corr = corr[target_col].drop(target_col).sort_values()\n",
    "colors_bar = [\"#e74c3c\" if v < 0 else \"#2ecc71\" for v in churn_corr.values]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.barh(churn_corr.index, churn_corr.values, color=colors_bar, edgecolor=\"black\", linewidth=0.5)\n",
    "plt.axvline(0, color=\"black\", linewidth=0.8)\n",
    "plt.title(\"Feature Correlation with Churn\", fontweight=\"bold\")\n",
    "plt.xlabel(\"Pearson Correlation\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation ranking:**",
    "- **Green bars (positive correlation = increases churn)**: `days_since_last_login` is the strongest driver. `payment_failures_180d` and `num_tickets_90d` also push churn up.",
    "- **Red bars (negative correlation = decreases churn)**: `sessions_per_week` and `avg_session_minutes` are the strongest protective factors.",
    "- Features near zero (`age`, `monthly_fee`, `discount_rate`) have weak linear relationships with churn individually, though they may still contribute in non-linear models like Random Forest or XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Categorical Feature Analysis",
    "",
    "We examine churn rates across categorical segments to identify high-risk customer profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "# Remove date columns if they slipped through\n",
    "cat_cols = [c for c in cat_cols if \"date\" not in c.lower()]\n",
    "print(f\"Categorical features ({len(cat_cols)}): {cat_cols}\")\n",
    "\n",
    "# Cardinality check\n",
    "for col in cat_cols:\n",
    "    n_unique = df[col].nunique()\n",
    "    flag = \" ** HIGH CARDINALITY **\" if n_unique >= 20 else \"\"\n",
    "    print(f\"  {col}: {n_unique} unique values{flag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cardinality check:** Most categorical features have low cardinality (3-6 unique values) and are safe for one-hot encoding. If `country` has 20+ unique values, it should be frequency-encoded or grouped to avoid dimensionality explosion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5a. Churn rate by category\n",
    "n_cols_plot = min(3, len(cat_cols))\n",
    "n_rows_plot = -(-len(cat_cols) // n_cols_plot)\n",
    "fig, axes = plt.subplots(n_rows_plot, n_cols_plot, figsize=(7 * n_cols_plot, 5 * n_rows_plot))\n",
    "if n_rows_plot * n_cols_plot == 1:\n",
    "    axes = [axes]\n",
    "else:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(cat_cols):\n",
    "    churn_rate = df.groupby(col)[target_col].mean().sort_values(ascending=False)\n",
    "    # For high-cardinality, show only top 15\n",
    "    if len(churn_rate) > 15:\n",
    "        churn_rate = churn_rate.head(15)\n",
    "    bars = axes[i].barh(churn_rate.index, churn_rate.values, color=\"#3498db\", edgecolor=\"black\", linewidth=0.5)\n",
    "    axes[i].set_title(f\"Churn Rate by {col}\", fontweight=\"bold\")\n",
    "    axes[i].set_xlabel(\"Churn Rate\")\n",
    "    axes[i].axvline(df[target_col].mean(), color=\"red\", linestyle=\"--\", label=\"Overall Avg\")\n",
    "    axes[i].legend(fontsize=8)\n",
    "\n",
    "for j in range(len(cat_cols), len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.suptitle(\"Churn Rate Across Categorical Features\", fontsize=14, fontweight=\"bold\", y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical churn rate insights:**",
    "- **`plan`**: Look for plan tiers with churn rates significantly above the red dashed line (overall average ~23%). Premium plans may retain better if they offer more value.",
    "- **`support_tier`**: Customers with \"none\" or lower support tiers may churn more, suggesting that access to support is a retention lever.",
    "- **`acquisition_channel`**: Some channels may bring in less committed customers. This is actionable for marketing spend optimization.",
    "- **`platform_primary`**: If one platform shows higher churn, it may indicate UX issues specific to that platform.",
    "- The red dashed line marks the overall churn rate - any bar extending beyond it represents a higher-risk segment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Outlier & Skewness Analysis",
    "",
    "Identifying skewed features helps decide whether log-transforms or robust scaling are needed before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6a. Skewness analysis\n",
    "skew_df = df[num_cols].skew().sort_values(ascending=False).reset_index()\n",
    "skew_df.columns = [\"Feature\", \"Skewness\"]\n",
    "skew_df[\"Abs_Skew\"] = skew_df[\"Skewness\"].abs()\n",
    "skew_df[\"Transform_Suggested\"] = skew_df[\"Abs_Skew\"].apply(\n",
    "    lambda x: \"Log Transform\" if x > 1.5 else (\"Moderate - Monitor\" if x > 0.75 else \"OK\")\n",
    ")\n",
    "\n",
    "display(skew_df.style.applymap(\n",
    "    lambda x: \"background-color: #f8d7da\" if x == \"Log Transform\"\n",
    "    else (\"background-color: #fff3cd\" if x == \"Moderate - Monitor\" else \"\"),\n",
    "    subset=[\"Transform_Suggested\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skewness table:** Features highlighted in red (skewness > 1.5) are heavily right-skewed and would benefit from a `log1p` transform for linear models. Tree-based models (Random Forest, XGBoost) are naturally robust to skewness, so transforms are optional for those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6b. Visualize the most skewed features\n",
    "highly_skewed = skew_df[skew_df[\"Abs_Skew\"] > 1.0][\"Feature\"].tolist()\n",
    "\n",
    "if highly_skewed:\n",
    "    n = len(highly_skewed)\n",
    "    fig, axes = plt.subplots(1, min(n, 4), figsize=(5 * min(n, 4), 4))\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    for i, col in enumerate(highly_skewed[:4]):\n",
    "        axes[i].hist(df[col].dropna(), bins=40, color=\"#9b59b6\", edgecolor=\"white\", alpha=0.8)\n",
    "        axes[i].set_title(f\"{col} (skew={df[col].skew():.2f})\", fontweight=\"bold\")\n",
    "        axes[i].set_ylabel(\"Count\")\n",
    "    plt.suptitle(\"Highly Skewed Features\", fontweight=\"bold\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No highly skewed features detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skewed feature visualization:** These histograms confirm the heavy right-skew - the vast majority of values cluster near zero, with a long tail of outliers. For example:",
    "- `payment_failures_180d`: Most customers have zero payment failures; a small number have several, and those customers are at higher churn risk.",
    "- `num_tickets_90d`: Similarly, most customers don't contact support, but those who do frequently tend to be dissatisfied.",
    "",
    "These features are naturally binary-like and may benefit from being converted to flags (e.g., `has_payment_issue = payment_failures > 0`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Engagement & Business Deep Dive",
    "",
    "Combining multiple features to understand the engagement-churn relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly fee vs engagement, colored by churn\n",
    "plt.figure(figsize=(10, 7))\n",
    "sample = df.sample(min(5000, len(df)), random_state=RANDOM_STATE)\n",
    "scatter = plt.scatter(\n",
    "    sample[\"monthly_fee\"], sample[\"sessions_per_week\"],\n",
    "    c=sample[target_col], cmap=\"RdYlGn_r\", alpha=0.4, s=15, edgecolors=\"none\"\n",
    ")\n",
    "plt.colorbar(scatter, label=\"Churned (1) vs Retained (0)\")\n",
    "plt.xlabel(\"Monthly Fee ($)\")\n",
    "plt.ylabel(\"Sessions per Week\")\n",
    "plt.title(\"Monthly Fee vs. Engagement (colored by churn)\", fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scatter plot insight:** The red-yellow cluster (churned customers) tends to sit in the lower-left region - these are customers paying moderate fees but not engaging with the product. Retained customers (green) are spread more broadly but cluster at higher session counts. This confirms that **engagement, not price, is the primary churn driver**. A customer paying $30/month who logs in 10 times weekly is far less likely to churn than one paying $15/month who logs in once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Insights for Modeling",
    "",
    "This section provides a structured handoff from EDA to the modeling pipeline.",
    "",
    "---",
    "",
    "### Columns to Drop",
    "| Column | Reason |",
    "|---|---|",
    "| `customer_id` | Identifier, not predictive |",
    "| `signup_date` | Raw date (extract features first) |",
    "| `last_active` | **Leakage** - reflects when customer churned |",
    "| `days_since_last_login` | **Leakage** - directly caused by churn event |",
    "| `month_last_active` | **Leakage** - encodes when churn happened |",
    "",
    "### Columns Needing Scaling",
    "- All numeric features need `StandardScaler` for Logistic Regression.",
    "- Tree-based models (RF, XGBoost) are scale-invariant.",
    "",
    "### Columns Needing Encoding",
    "| Column | Strategy |",
    "|---|---|",
    "| `plan`, `platform_primary`, `support_tier`, `acquisition_channel` | One-Hot Encoding (low cardinality) |",
    "| `country` | Frequency Encoding (high cardinality, 20+ unique values) |",
    "| `coupon_used_signup` | Already binary (0/1) - no encoding needed |",
    "",
    "### Highly Predictive Features (from correlation & visual inspection)",
    "1. `sessions_per_week` - strong negative correlation with churn",
    "2. `avg_session_minutes` - low engagement = high churn risk",
    "3. `days_since_last_login` - **leaky, must drop**, but validates domain logic",
    "4. `payment_failures_180d` - billing friction drives churn",
    "5. `num_tickets_90d` - support load signals dissatisfaction",
    "",
    "### Class Imbalance Handling",
    "- 77/23 split (3.3:1 ratio) - moderate imbalance.",
    "- **Recommended**: `class_weight='balanced'` or `scale_pos_weight` for XGBoost.",
    "- Consider threshold tuning (lowering from 0.5 to ~0.35) to boost Recall.",
    "",
    "### Feature Engineering Ideas",
    "1. `total_engagement` = `sessions_per_week` x `avg_session_minutes`",
    "2. `has_payment_issue` = binary flag from `payment_failures_180d > 0`",
    "3. `has_tickets` = binary flag from `num_tickets_90d > 0`",
    "4. `signup_month` / `signup_dayofweek` from `signup_date`",
    "5. `tenure_days` from `last_active - signup_date` (use carefully - partially leaky)",
    "",
    "---",
    "",
    "> **Next Step**: Open `cc_modeling_pipeline.ipynb` for the full production-grade modeling pipeline with proper leakage removal, sklearn Pipelines, hyperparameter tuning, SHAP explainability, and business strategy."
   ]
  }
 ]
}